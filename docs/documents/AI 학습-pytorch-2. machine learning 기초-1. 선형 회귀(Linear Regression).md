---
title: 
parent: Documents
layout: default
---

# 

> Source: `AI 학습/pytorch/2. machine learning 기초/1. 선형 회귀(Linear Regression).md`

---

출처 : https://wikidocs.net/53560


1. 훈련 데이터셋
    - x_train은 공부한 시간, y_train은 그에 맵핑되는 점수 에 대한 데이터를 구성해보자
    $$X_{train} = \begin{pmatrix} 1\\2\\3 \end{pmatrix}$$
    $$Y_{train} = \begin{pmatrix} 2\\4\\6 \end{pmatrix}$$
2. 가설 / 오차
    - 선형 회귀의 가설은 보통 아래의 식으로 표현된다.
    $$H(x) = W_x + b$$
    - 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)

    <img src="./image/오차탐색1.png">

    - 위 그림은 실제값(4개의 점)에서 예측값에 대한 차이를 구하여는 이미지이다.
    - 직선의 식은 $$y = 13x + 1$$ 이며 각 오차는 다음과 같다.
        |Hours|2|3|4|5|
        |---|---|---|---|---|
        |실제값|25|50|42|61|
        |예측값|27|40|53|66|
        |오차|-2|10|-9|-5|
    - 오차 분석 시 오차를 모두 더해야하는데 음수값을 고려하여 모두 제곱 후 더하면 210, 제곱 후 평균은 52.5이 나온다.
    - 이 평균 제곱오차를 최소화해야한다..

3. 옵티마이저 - 경사하강법
    - 옵티마이저 : 최적화 알고리즘
    - 경사하강법은 옵티마이저의 기본적인 방법으로 기울기를 변경하며 오차를 최소화한다.
    - 오차가 최소값이 될 기울기 W를 찾으려면 비용-기울기 간 그래프의 기울기가 0이 될 값을 구하면 될 것이다.

    <img src="./image/접선의기울기1.png">

    - 결과적으로 이를 수식으로 풀면 다음과 같다. 여기서 alpha는 학습률을 의미한다.
    $$W =: W - \alpha{\frac{\partial}{\partial W}}cost(W)$$


4. pytorch로 구현
    ```py
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    # 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.
    torch.manual_seed(1)

    x_train = torch.FloatTensor([[1], [2], [3]])
    y_train = torch.FloatTensor([[2], [4], [6]])
    print(x_train)
    print(x_train.shape)
    print(y_train)
    print(y_train.shape)
    ```
    ```
    tensor([[1.],
            [2.],
            [3.]])
    torch.Size([3, 1])
    tensor([[2.],
            [4.],
            [6.]])
    torch.Size([3, 1])
    ```

    - 가중치를 0으로 둠. requires_grad 는 학습을 통해 값이 변경되는 변수임을 명시
    ```py
    W = torch.zeros(1, requires_grad=True) 
    print(W) 
    b = torch.zeros(1, requires_grad=True)
    print(b)
    ```
    ```
    tensor([0.], requires_grad=True)
    tensor([0.], requires_grad=True)
    ```

    - 직선 방정식으로 구성되는 가설을 세운다.
    ```py
    hypothesis = x_train * W + b
    print(hypothesis)
    ```
    - 이에 따른 평균 제곱 오차를 계산한다.
    ```py
    cost = torch.mean((hypothesis - y_train) ** 2) 
    print(cost)
    ```
    ```
    tensor(18.6667, grad_fn=<MeanBackward0>)
    ```

    - 경사하강법을 구현한다.
    - 대략 epoch 2000번을 돌리면서 최적값을 구한다. SGD는 경사하강법의 일종을 의미하며, lr은 학습률을 의미한다.
    ```py
    # 모델 초기화
    W = torch.zeros(1, requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    # optimizer 설정
    optimizer = optim.SGD([W, b], lr=0.01)
    nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복
    for epoch in range(nb_epochs + 1):
        # H(x) 계산
        hypothesis = x_train * W + b

        # cost 계산
        cost = torch.mean((hypothesis - y_train) ** 2)

        # cost로 H(x) 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        # 100번마다 로그 출력
        if epoch % 100 == 0:
            print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
                epoch, nb_epochs, W.item(), b.item(), cost.item()
            ))
    ```

    - optimizer.zero_grad()
        * pytorch는 미분 결과(backward())의 기울기를 누적시킨다. zero_grad 함수는 이를 초기화한다.

    - torch.manual_seed()
        * 랜덤 시드 고정

    - 자동 미분
        * 다음 수식을 가지고 실습
        * 값이 2엔 텐서를 미분 후 처리한 결과를 나타낸다.
        $$2w^2 + 5$$
        ```py
        w = torch.tensor(2.0, requires_grad=True)
        y = w**2
        z = 2*y + 5
        z.backward()
        print('수식을 w로 미분한 값 : {}'.format(w.grad))
        ```
        ```
        수식을 w로 미분한 값 : 8.0
        ```