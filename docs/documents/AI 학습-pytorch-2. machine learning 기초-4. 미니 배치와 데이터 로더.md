---
title: 
parent: Documents
layout: default
---

# 

> Source: `AI 학습/pytorch/2. machine learning 기초/4. 미니 배치와 데이터 로더.md`

---

출처 : https://wikidocs.net/55580


1. 미니 배치와 배치 크기(Mini Batch and Batch Size)
    - 미니 배치 : 데이터(1epoch)를 일정 크기로 나누어 연산. 이;ㄹ반적으로 2의 제곱 형태로 구성
    - iteration : total Data / batchSize = iteration


2. 데이터 로드하기
    ```py
    import torch
    import torch.nn as nn
    import torch.nn.functional as F

    from torch.utils.data import TensorDataset # 텐서데이터셋
    from torch.utils.data import DataLoader # 데이터로더
    #torch.manual_seed(1)

    x_train  =  torch.FloatTensor([[73,  80,  75], 
                                [93,  88,  93], 
                                [89,  91,  90], 
                                [96,  98,  100],   
                                [73,  66,  70]])  
    y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])
    dataset = TensorDataset(x_train, y_train)

    # shuffle : 데이터셋 랜덤처리
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
    model = nn.Linear(3,1)
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) 

    nb_epochs = 20
    for epoch in range(nb_epochs + 1):
    for batch_idx, samples in enumerate(dataloader):
        # print(batch_idx)
        # print(samples)
        x_train, y_train = samples
        # H(x) 계산
        prediction = model(x_train)

        # cost 계산
        cost = F.mse_loss(prediction, y_train)

        # cost로 H(x) 계산
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(
            epoch, nb_epochs, batch_idx+1, len(dataloader),
            cost.item()
            ))
    ```



3. 커스텀 데이터셋으로 선형 회귀 구현하기
    ```py
    import torch
    import torch.nn.functional as F
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader

    # Dataset 상속
    class CustomDataset(Dataset): 
    def __init__(self):
        self.x_data = [[73, 80, 75],
                    [93, 88, 93],
                    [89, 91, 90],
                    [96, 98, 100],
                    [73, 66, 70]]
        self.y_data = [[152], [185], [180], [196], [142]]

    # 총 데이터의 개수를 리턴
    def __len__(self): 
        return len(self.x_data)

    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴
    def __getitem__(self, idx): 
        x = torch.FloatTensor(self.x_data[idx])
        y = torch.FloatTensor(self.y_data[idx])
        return x, y

    dataset = CustomDataset()
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
    model = torch.nn.Linear(3,1)
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) 

    nb_epochs = 20
    for epoch in range(nb_epochs + 1):
    for batch_idx, samples in enumerate(dataloader):
        # print(batch_idx)
        # print(samples)
        x_train, y_train = samples
        # H(x) 계산
        prediction = model(x_train)

        # cost 계산
        cost = F.mse_loss(prediction, y_train)

        # cost로 H(x) 계산 
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(
            epoch, nb_epochs, batch_idx+1, len(dataloader),
            cost.item()
            ))
    ```


