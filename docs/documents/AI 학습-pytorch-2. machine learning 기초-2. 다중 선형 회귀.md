---
title: 
parent: Documents
layout: default
---

# 

> Source: `AI 학습/pytorch/2. machine learning 기초/2. 다중 선형 회귀.md`

---

출처 : https://wikidocs.net/54841

1. 다중 선형 회귀
    - 다수의 x값으로 y를 예측
    - x가 3개인 수식에 대한 예제를 구성한다. 이를 식으로 구성하면
    $$H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b$$
    - 이를 pytorch로 구현해보자. 우선 import, 랜덤시드고정부터 한다.
    ```py
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim

    torch.manual_seed(1)
    ```
    - 훈련 데이터를 넣는다.
    ```py
    x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
    x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
    x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
    y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
    ```
    - 가중치 및 b를 선언한다.
    ```py
    w1 = torch.zeros(1, requires_grad=True)
    w2 = torch.zeros(1, requires_grad=True)
    w3 = torch.zeros(1, requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    ```
    - 이를 동일한 형태로 학습을 시킨다. 1e-5는 1 * 10^-5 = 0.00001이겠지..
    ```py
    # optimizer 설정
    optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)
    nb_epochs = 2000
    for epoch in range(nb_epochs + 1):
        # H(x) 계산
        hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b

        # cost 계산
        cost = torch.mean((hypothesis - y_train) ** 2)

        # cost로 H(x) 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        # 100번마다 로그 출력
        if epoch % 100 == 0:
            print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
                epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
            ))
    ```

2. 벡터와 행렬 연산으로 바꾸기
    - 이 식을 벡터연산으로 바꿔보자.
    $$H(x) = w_1x_1 + w_2x_2 + w_3x_3$$
    - 이는 아래 식처럼 바꿀수 있다.
    $$\begin{pmatrix}x_1 & x_2 & x_3\end{pmatrix} * \begin{pmatrix}w_1 \\ w_2 \\ w_3\end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3)$$
    - 이 두 벡터를 X와 W로 표현하면
    $$H(X) = XW$$
    - 그럼 이를 5개 샘플의 경우로 생각해보자.
    $$\begin{pmatrix}
    x_{11} & x_{12} & x_{13}\\
    x_{21} & x_{22} & x_{23}\\
    x_{31} & x_{32} & x_{33}\\
    x_{41} & x_{42} & x_{43}\\
    x_{51} & x_{52} & x_{53}\\
    \end{pmatrix}
    \begin{pmatrix}
    w_1\\w_2\\w_3
    \end{pmatrix} + 
    \begin{pmatrix}
    b\\b\\b\\b\\b
    \end{pmatrix} = 
    \begin{pmatrix}
    x_{11}w_1 + x_{12}w_2 + x_{13}w_3 + b\\
    x_{21}w_1 + x_{22}w_2 + x_{23}w_3 + b\\
    x_{31}w_1 + x_{32}w_2 + x_{33}w_3 + b\\
    x_{41}w_1 + x_{42}w_2 + x_{43}w_3 + b\\
    x_{51}w_1 + x_{52}w_2 + x_{53}w_3 + b\\
    \end{pmatrix}$$

    - 이를 pytorch로 구현한다.
    ```py
    x_train  =  torch.FloatTensor([[73,  80,  75], 
                                [93,  88,  93], 
                                [89,  91,  80], 
                                [96,  98,  100],   
                                [73,  66,  70]])  
    y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])
    W = torch.zeros((3, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    # optimizer 설정
    optimizer = optim.SGD([W, b], lr=1e-5)
    nb_epochs = 20
    for epoch in range(nb_epochs + 1):
        # H(x) 계산
        # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.
        hypothesis = x_train.matmul(W) + b

        # cost 계산
        cost = torch.mean((hypothesis - y_train) ** 2)

        # cost로 H(x) 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()
        ))
    ```
    - 예측하기
    ```py
    with torch.no_grad():
        new_input = torch.FloatTensor([[75, 85, 72]])  # 예측하고 싶은 임의의 입력
        prediction = new_input.matmul(W) + b
        print('Predicted value for input {}: {}'.format(new_input.squeeze().tolist(), prediction.item()))
    ```