---
title: 
parent: Documents
layout: default
---

# 

> Source: `AI 학습/pytorch/1. pytorch기초/1. pytorch  기초.md`

---

출처 : https://wikidocs.net/52460

>1. pytorch 패키지 기본 구성
* torch : 메인 네임스페이스입니다. 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조를 가집니다.
* torch.autogard : 자동 미분을 위한 함수들이 포함되어져 있습니다. 자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 'Function' 등이 포함되어져 있습니다.
* torch.nn : 신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있습니다. 예를 들어 RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다.
* torch.optim : 확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있습니다.
* torch.utils.data : SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어져 있습니다.
* torch.onnix : ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷입니다.

_____



>2. tensor 조작하기
* 벡터, 행렬 그리고 텐서(Vector, Matrix and Tensor)
    - 2D Tensor(Typical Simple Setting) : |t| = (Batch size, dim)
    - 3D Tensor(Typical Computer Vision / 비전 분야에서의 3차원 텐서) : |t| = (Batch size, width, height)
    - 3D Tensor(Typical Natural Language Processing / NLP 분야에서의 3차원 텐서) : |t| = (Batch size, length, dim)
        * 자연어처리는 보통 이쪽을 사용한다. 
        * 예시
            1. [[나는 사과를 좋아해],<br>
                [나는 바나나를 좋아해],<br>
                [나는 사과를 싫어해],<br>
                [나는 바나나를 싫어해]]<br>
                이 행렬을 단어로 나눈다.
            2. [['나는', '사과를', '좋아해'],<br>
                ['나는', '바나나를', '좋아해'],<br>
                ['나는', '사과를', '싫어해'],<br>
                ['나는', '바나나를', '싫어해']]<br>
                이를 단어기준으로 벡터로 변환한다.
            3. 아래 벡터 데이터를 기준으로 벡터변환을 실시한다.
                * '나는' = [0.1, 0.2, 0.9]
                * '사과를' = [0.3, 0.5, 0.1]
                * '바나나를' = [0.3, 0.5, 0.2]
                * '좋아해' = [0.7, 0.6, 0.5]
                * '싫어해' = [0.5, 0.6, 0.7]
            4. 결과는 다음과 같다.<br>
                [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],<br>
                [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],<br>
                [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],<br>
                [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]<br>
                이는 4 * 3 * 3 tensor이다.
            5. 이를 batchSize를 2로 변환 시 다음과 같이 될 것이다.<br>
                * 첫번째 배치 #1<br>
                [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],<br>
                [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]
                * 두번째 배치 #2<br>
                [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],<br>
                [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]<br>


* 넘파이로 텐서 만들기(벡터와 행렬 만들기)
    - numpy import
    ```py
    import numpy as np
    ```
    - 1차원 tensor 선언
    ```py
    t = np.array([0., 1., 2., 3., 4., 5., 6.])
    # 파이썬으로 설명하면 List를 생성해서 np.array로 1차원 array로 변환함.
    print(t)
    print('Rank of t: ', t.ndim)
    print('Shape of t: ', t.shape)
    ```
    ```
    [0. 1. 2. 3. 4. 5. 6.]
    Rank of t:  1
    Shape of t:  (7,)
    ```
    - numpy index
        * 인덱스는 0부터 시작, 음수의 경우 맨 뒤부터 역순
        * 범위지정으로 원소호출이 가능. t[2:5] 의 형태로 범위를 가져온다. 다만 선언된 끝 번호는 결과에 포함되지 않는다.
        ```py
        print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1])
        ```
        ```
        t[2:5] t[4:-1]  =  [2. 3. 4.] [4. 5.]
        ```
        * 시작번호나 끝 번호를 생략도 가능한데 생략 시 처음부터 또는 끝까지 뽑아낸다.
        ```py
        print('t[:2] t[3:]     = ', t[:2], t[3:])
        ```
        ```
        t[:2] t[3:]     =  [0. 1.] [3. 4. 5. 6.]
        ```
    - 2차원 tensor 선언
    ```py
    t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
    print('Rank  of t: ', t.ndim)
    print('Shape of t: ', t.shape)
    ```
    ```
    [[ 1.  2.  3.]
    [ 4.  5.  6.]
    [ 7.  8.  9.]
    [10. 11. 12.]]
    Rank  of t:  2
    Shape of t:  (4, 3)
    ```

* pytorch tensor 선언하기
    - pytorch import
    ```py
    import torch
    ```
    - 1차원 tensor 선언
    ```py
    t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
    print(t)
    print(t.dim())  # rank. 즉, 차원
    print(t.shape)  # shape
    print(t.size()) # shape
    ```
    ```
    tensor([0., 1., 2., 3., 4., 5., 6.])
    1
    torch.Size([7])
    torch.Size([7])
    ```
    - pytorch에서도 numpy와 같은 방법으로 인덱싱 처리가 가능하다.
    ```py
    print(t[0], t[1], t[-1])  # 인덱스로 접근
    print(t[2:5], t[4:-1])    # 슬라이싱
    print(t[:2], t[3:])       # 슬라이싱
    ```
    ```
    tensor(0.) tensor(1.) tensor(6.)
    tensor([2., 3., 4.]) tensor([4., 5.])
    tensor([0., 1.]) tensor([3., 4., 5., 6.])
    ```
    - 2차원 tensor 선언
    ```py
    t = torch.FloatTensor([[1., 2., 3.],
                        [4., 5., 6.],
                        [7., 8., 9.],
                        [10., 11., 12.]
                        ])
    print(t)
    print(t.dim())  # rank. 즉, 차원
    print(t.size()) # shape
    ```
    ```
    tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.],
        [10., 11., 12.]])
    2
    torch.Size([4, 3])
    ```
    - 이를 슬라이스해보자.
    ```py
    print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.
    print(t[:, 1].size()) # ↑ 위의 경우의 크기
    ```
    ```
    tensor([ 2.,  5.,  8., 11.])
    torch.Size([4])
    ```
    ```py
    print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.
    ```
    ```
    tensor([[ 1.,  2.],
        [ 4.,  5.],
        [ 7.,  8.],
        [10., 11.]])
    ```

* broadcasting
    - 크기가 다른 행렬을 연산할 때 크기를 일치하도록 맞추는 작업
        * 같은 크기일 때의 tensor의 연산
        ```py
        m1 = torch.FloatTensor([[3, 3]])
        m2 = torch.FloatTensor([[2, 2]])
        print(m1 + m2)
        ```
        ```
        tensor([[5., 5.]])
        ```
        * 다른 크기일 때의 tensor의 연산1(vector + sclara)
        ```py
        # Vector + scalar
        m1 = torch.FloatTensor([[1, 2]])
        m2 = torch.FloatTensor([3]) # [3] -> [3, 3]
        print(m1 + m2)
        ```
        ```
        tensor([[4., 5.]])
        ```
        * 다른 크기일 때의 tensor의 연산2(형태가 다른 두 벡터)
        ```py
        # 2 x 1 Vector + 1 x 2 Vector
        m1 = torch.FloatTensor([[1, 2]])
        m2 = torch.FloatTensor([[3], [4]])
        print(m1 + m2)
        ```
        ```
        tensor([4., 5.],
        [5., 6.]])
        ```
        - [1, 2] => [[1, 2],[1, 2]]
        - [3], [4] => [[3, 4],[3, 4]]

* 곱셈
    - 행렬곱셈 Matrix Multiplication
    ```py
    m1 = torch.FloatTensor([[1, 2], [3, 4]])
    m2 = torch.FloatTensor([[1], [2]])
    print('Shape of Matrix 1: ', m1.shape) # 2 x 2
    print('Shape of Matrix 2: ', m2.shape) # 2 x 1
    print(m1.matmul(m2)) # 2 x 1
    ```
    ```
    Shape of Matrix 1:  torch.Size([2, 2])
    Shape of Matrix 2:  torch.Size([2, 1])
    tensor([[ 5.],
            [11.]])
    ```
    - 원소별 곱셈 Multiplication
    ```py
    m1 = torch.FloatTensor([[1, 2], [3, 4]])
    m2 = torch.FloatTensor([[1], [2]])
    print('Shape of Matrix 1: ', m1.shape) # 2 x 2
    print('Shape of Matrix 2: ', m2.shape) # 2 x 1
    print(m1 * m2) # 2 x 2
    print(m1.mul(m2))
    ```
    ```
    Shape of Matrix 1:  torch.Size([2, 2])
    Shape of Matrix 2:  torch.Size([2, 1])
    tensor([[1., 2.],
            [6., 8.]])
    tensor([[1., 2.],
            [6., 8.]])
    ```

- 평균
    ```py
    t = torch.FloatTensor([1, 2])
    print(t.mean())
    ```
    ```
    tensor(1.5000)
    ```
    ```py
    t = torch.FloatTensor([[1, 2], [3, 4]])
    print(t)
    print(t.mean())
    ```
    ```
    tensor([[1., 2.],
            [3., 4.]])
    tensor(2.5000)
    ```
    * t.mean(dim = 0)인 경우 첫번째 차원(행) 을 제거 후 평균을 계산한다. 따라서 열의 평균을 구한다.
    ```py
    print(t.mean(dim=0))
    ```
    ```
    tensor([2., 3.])
    ```
    * t.mean(dim = 1)인 경우 두번째 차원(열) 을 제거 후 평균을 계산한다. 따라서 행의 평균을 구한다.
    ```py
    print(t.mean(dim=1))
    ```
    ```
    tensor([1.5000, 3.5000])
    ```
- 덧셈
    - 연산 방법과 인자가 의미하는 내용은 mean과 같다.
    ```py
    t = torch.FloatTensor([[1, 2], [3, 4]])
    print(t)
    print(t.sum()) # 단순히 원소 전체의 덧셈을 수행
    print(t.sum(dim=0)) # 행을 제거
    print(t.sum(dim=1)) # 열을 제거
    print(t.sum(dim=-1)) # 열을 제거
    ```
    ```
    tensor([[1., 2.],
            [3., 4.]])
    tensor(10.)
    tensor([4., 6.])
    tensor([3., 7.])
    tensor([3., 7.])
    ```

- 최대(Max)와 아그맥스(ArgMax)
    * 최대(Max)는 원소의 최대값을 리턴하고, 아그맥스(ArgMax)는 최대값을 가진 인덱스를 리턴합니다.
    ```py
    t = torch.FloatTensor([[1, 2], [3, 4]])
    print(t)
    print(t.max()) # Returns one value: max
    print(t.max(dim=0)) # Returns two values: max and argmax
    ```
    ```
    tensor([[1., 2.],
            [3., 4.]])
    tensor(4.)
    torch.return_types.max(
    values=tensor([3., 4.]),
    indices=tensor([1, 1]))
    ```
    * max에 dim을 줄 경우 argmax도 같이 출력이 된다. 위 예제의 두번째 결과는 최대값의 인덱스값이다. max 또는 argmax만 받으려면 인덱스 처리를 해주면 된다.
    ```py
    print('Max: ', t.max(dim=0)[0])
    print('Argmax: ', t.max(dim=0)[1])
    ```
    ```
    Max:  tensor([3., 4.])
    Argmax:  tensor([1, 1])
    ```


- 뷰
    * tensor의 크기를 변경한다.
    * 3차원 텐서에서 2차원 텐서로 변경
        ```py
        t = np.array([[[0, 1, 2],
                    [3, 4, 5]],
                    [[6, 7, 8],
                    [9, 10, 11]]])
        ft = torch.FloatTensor(t)
        print(ft.shape)
        ```
        ```
        torch.Size([2, 2, 3])
        ```
        - 이를 2차원으로 변경한다. 여기서 -1의 의미는 pytorch에 맡긴다는 의미, 3은 두번째 차원의 길이는 3으로 한다는 의미. 따라서 view([-1, 3]) 은 2차원으로 구성하되 두번째 열의 수는 3으로 구성해라 이다.
        ```py
        print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경
        print(ft.view([-1, 3]).shape)
        ```
        ```
        tensor([[ 0.,  1.,  2.],
                [ 3.,  4.,  5.],
                [ 6.,  7.,  8.],
                [ 9., 10., 11.]])
        torch.Size([4, 3])
        ```
        - 규칙
            * view는 기본적으로 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지되어야 합니다.
            * 파이토치의 view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추합니다.
    * 3차원 텐서의 크기 변경
        - 2 * 2 * 3인 크기의 3차원 tensor를 ? * 1 * 3 크기로 변경시 ?는 4가 되어야 한다.
        ```py
        print(ft.view([-1, 1, 3]))
        print(ft.view([-1, 1, 3]).shape)
        ```
        ```
        tensor([[[ 0.,  1.,  2.]],

            [[ 3.,  4.,  5.]],

            [[ 6.,  7.,  8.]],

            [[ 9., 10., 11.]]])
        torch.Size([4, 1, 3])
        ```


* 스퀴즈
    - 차원이 1인 경우 해당 차원을 제거한다.
    ```py
    ft = torch.FloatTensor([[0], [1], [2]])
    print(ft)
    print(ft.squeeze())
    print(ft.squeeze().shape)
    ```
    ```
    tensor([[0.],
            [1.],
            [2.]])
    tensor([0., 1., 2.])
    torch.Size([3])
    ```
* 언스퀴즈(Unsqueeze) - 특정 위치에 1인 차원을 추가한다.
    - 특정 위치에 1인 차원을 추가할 수 있습니다.
    - 이 예제는 0에 차원을 하나 추가한다.  unsqueeze(0) 은 view(1, -1) 로도 구현 가능하다.
    ```py
    ft = torch.Tensor([0, 1, 2])
    print(ft.shape)
    print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.
    print(ft.unsqueeze(0).shape)
    print(ft.view(1, -1))
    print(ft.view(1, -1).shape)
    ```
    ```
    torch.Size([3])
    tensor([[0., 1., 2.]])
    torch.Size([1, 3])
    tensor([[0., 1., 2.]])
    torch.Size([1, 3])
    ```
    * 인덱스 1에 차뭔을 추가하면 3 -> (3, 1)로 바뀌게 된다. -1을 인자로 주면 마지막에 차원을 추가한 상태가 된다.
    ```py
    print(ft.unsqueeze(1))
    print(ft.unsqueeze(1).shape)
    ```
    ```
    tensor([[0.],
            [1.],
            [2.]])
    torch.Size([3, 1])
    ```
    ```py
    print(ft.unsqueeze(-1))
    print(ft.unsqueeze(-1).shape)
    ```
    ```
    tensor([[0.],
        [1.],
        [2.]])
    torch.Size([3, 1])
    ```


* 형변환
    - 아래 예제는 long 자료형의 tensor를 float형태로 형변환하는 예제이다.
    ```py
    lt = torch.LongTensor([1, 2, 3, 4])
    print(lt)
    print(lt.float())
    ```
    ```
    tensor([1, 2, 3, 4])
    tensor([1., 2., 3., 4.])
    ```

* 연결하기
    - 두 텐서를 연결하는 예제이다. 여기서 dim 인자는 연결하는 대상 차원이다. 
    - 아래 예제에서 0으로 줄 때는 4 * 2 형태가, 1로 줄때는 2 * 4가 됨을 확인할수있다. 
    ```py
    x = torch.FloatTensor([[1, 2], [3, 4]])
    y = torch.FloatTensor([[5, 6], [7, 8]])
    print(torch.cat([x, y], dim=0))
    print(torch.cat([x, y], dim=1))
    ```
    ```
    tensor([[1., 2.],
            [3., 4.],
            [5., 6.],
            [7., 8.]])
    tensor([[1., 2., 5., 6.],
            [3., 4., 7., 8.]])
    ```

* 스택킹
    - 데이터를 순차적으로 쌓게 하는 연산이다.
    ```py
    x = torch.FloatTensor([1, 4])
    y = torch.FloatTensor([2, 5])
    z = torch.FloatTensor([3, 6])
    print(torch.stack([x, y, z]))
    ```
    ```
    tensor([[1., 4.],
            [2., 5.],
            [3., 6.]])
    ```
    - 위 예제는 다음 명령어와 동일한 역할을 한다.
    ```py
    print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))
    ```
    - stack 명령어에 dim 파라미터를 던져주면 지정한 차원 대상으로 증가하게 한다.
    ```py
    print(torch.stack([x, y, z], dim=1))
    ```
    ```
    tensor([[1., 2., 3.],
            [4., 5., 6.]])
    ```

* ones_like와 zeros_like - 0으로 채워진 텐서와 1로 채워진 텐서
    ```py
    x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
    print(x)
    print(torch.ones_like(x))
    print(torch.zeros_like(x))
    ```
    ```
    tensor([[0., 1., 2.],
            [2., 1., 0.]])
    tensor([[1., 1., 1.],
            [1., 1., 1.]])
    tensor([[0., 0., 0.],
            [0., 0., 0.]])
    ```



* 덮어쓰기 연산
    - 연산자에 _ 를 붙이면 기존 값이 변경이 된다,
    ```py
    x = torch.FloatTensor([[1, 2], [3, 4]])
    print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력
    print(x) # 기존의 값 출력
    print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력
    print(x) # 기존의 값 출력
    ```
    ```
    tensor([[2., 4.],
            [6., 8.]])
    tensor([[1., 2.],
            [3., 4.]])
    tensor([[2., 4.],
            [6., 8.]])
    tensor([[2., 4.],
            [6., 8.]])
    ```