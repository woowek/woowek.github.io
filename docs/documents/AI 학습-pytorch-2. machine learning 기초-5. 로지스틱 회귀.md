---
title: 
parent: Documents
layout: default
---

# 

> Source: `AI 학습/pytorch/2. machine learning 기초/5. 로지스틱 회귀.md`

---

출처 : https://wikidocs.net/57805

1. 이진 분류

    <img src="./image/로지스틱회귀.png">

    - 이 경우 직선으로는 표현이 불가능. 따라서 아래의 형태로 표현
    $$H(x) = f(W_x + b)$$
    - 이진 분류를 풀기 위한 알고리즘이 로지스틱 회귀
    - 여기서 일반적으로 사용하는 f가 시그모이드 함수


2. 시그모이드 함수
    $$H(x) = sigmoid(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)$$
    - python 예제
    ```py
    import numpy as np # 넘파이 사용
    import matplotlib.pyplot as plt# 맷플롯립사용

    def sigmoid(x): # 시그모이드 함수 정의
        return 1/(1+np.exp(-x))

    x = np.arange(-5.0, 5.0, 0.1)
    y = sigmoid(x)

    plt.plot(x, y, 'g')
    plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
    plt.title('Sigmoid Function')
    plt.savefig("sigmoid.png")
    ```
    <img src="./image/sigmoid.png">
    - 결과값은 0과 1 사이 값을 가짐


3. pytorch로 로지스틱 회귀 구현하기
    ```py
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    torch.manual_seed(1)


    x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
    y_data = [[0], [0], [0], [1], [1], [1]]
    x_train = torch.FloatTensor(x_data)
    y_train = torch.FloatTensor(y_data)

    W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1
    b = torch.zeros(1, requires_grad=True)

    hypothesis = torch.sigmoid(x_train.matmul(W) + b)
    # optimizer 설정
    optimizer = optim.SGD([W, b], lr=1)

    nb_epochs = 1000
    for epoch in range(nb_epochs + 1):

        # Cost 계산
        hypothesis = torch.sigmoid(x_train.matmul(W) + b)
        cost = -(y_train * torch.log(hypothesis) + 
                (1 - y_train) * torch.log(1 - hypothesis)).mean()

        # cost로 H(x) 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        # 100번마다 로그 출력
        if epoch % 100 == 0:
            print('Epoch {:4d}/{} Cost: {:.6f}'.format(
                epoch, nb_epochs, cost.item()
            ))
    ```
    ```py
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    torch.manual_seed(1)


    x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
    y_data = [[0], [0], [0], [1], [1], [1]]
    x_train = torch.FloatTensor(x_data)
    y_train = torch.FloatTensor(y_data)

    model = nn.Sequential(
    nn.Linear(2, 1), # input_dim = 2, output_dim = 1
    nn.Sigmoid() # 출력은 시그모이드 함수를 거친다
    )
    model(x_train)

    # optimizer 설정
    optimizer = optim.SGD(model.parameters(), lr=1)

    nb_epochs = 1000
    for epoch in range(nb_epochs + 1):

        # H(x) 계산
        hypothesis = model(x_train)

        # cost 계산
        cost = F.binary_cross_entropy(hypothesis, y_train)

        # cost로 H(x) 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        # 20번마다 로그 출력
        if epoch % 10 == 0:
            prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주
            correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주
            accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산
            print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력
                epoch, nb_epochs, cost.item(), accuracy * 100,
            ))
    ```

4. 클래스로 구현하기
    ```py
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    torch.manual_seed(1)

    x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
    y_data = [[0], [0], [0], [1], [1], [1]]
    x_train = torch.FloatTensor(x_data)
    y_train = torch.FloatTensor(y_data)

    class BinaryClassifier(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(2, 1)
            self.sigmoid = nn.Sigmoid()

        def forward(self, x):
            return self.sigmoid(self.linear(x))

    model = BinaryClassifier()

    # optimizer 설정
    optimizer = optim.SGD(model.parameters(), lr=1)

    nb_epochs = 1000
    for epoch in range(nb_epochs + 1):

        # H(x) 계산
        hypothesis = model(x_train)

        # cost 계산
        cost = F.binary_cross_entropy(hypothesis, y_train)

        # cost로 H(x) 개선
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        # 20번마다 로그 출력
        if epoch % 10 == 0:
            prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주
            correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주
            accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산
            print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력
                epoch, nb_epochs, cost.item(), accuracy * 100,
            ))
    ```

